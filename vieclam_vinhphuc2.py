import requests
from bs4 import BeautifulSoup
import csv
import re

BASE_URL = "https://vinhphuc.work"

def crawl_jobs_in_page(page):
    print(f"ğŸ”„ Äang xá»­ lÃ½ trang {page}...")
    url = f"{BASE_URL}/tin-tuyen-dung/page/{page}"
    resp = requests.get(url)
    if resp.status_code != 200:
        print(f"âŒ KhÃ´ng thá»ƒ táº£i trang {page}")
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    job_items = soup.select("div.item.p-2.rounded")

    results = []
    for item in job_items:
        # Láº¥y link chi tiáº¿t vÃ  tiÃªu Ä‘á»
        title_tag = item.select_one("a.title")
        if not title_tag:
            continue
        detail_url = title_tag["href"]
        title = title_tag.get_text(strip=True)

        # TÃªn cÃ´ng ty
        company_tag = item.select_one("strong a")
        company = company_tag.get_text(strip=True) if company_tag else ""

        # Má»©c lÆ°Æ¡ng, háº¡n ná»™p, Ä‘á»‹a Ä‘iá»ƒm
        info_spans = item.select("div.row-cols-3 span.form-text")
        salary = info_spans[0].get_text(strip=True) if len(info_spans) > 0 else ""
        deadline = info_spans[1].get_text(strip=True) if len(info_spans) > 1 else ""
        location = info_spans[2].get_text(strip=True) if len(info_spans) > 2 else ""

        # Truy cáº­p trang chi tiáº¿t Ä‘á»ƒ láº¥y pháº§n LiÃªn há»‡
        r2 = requests.get(detail_url)
        if r2.status_code != 200:
            lien_he_text = "(KhÃ´ng táº£i Ä‘Æ°á»£c chi tiáº¿t)"
        else:
            s2 = BeautifulSoup(r2.text, "html.parser")
            header = s2.find("h3", id="lien-he")
            lien_he_text = ""
            if header:
                for sib in header.find_next_siblings():
                    if sib.name and re.match(r"h[1-3]", sib.name):
                        break
                    lien_he_text += sib.get_text(" ", strip=True) + "\n"
            lien_he_text = lien_he_text.strip()

        results.append({
            "title": title,
            "url": detail_url,
            "company": company,
            "salary": salary,
            "deadline": deadline,
            "location": location,
            "lien_he": lien_he_text
        })

    return results


# ==== âš™ï¸ Thiáº¿t láº­p khoáº£ng trang ====
start_page = 401
end_page = 450   # ğŸ‘‰ Thay Ä‘á»•i táº¡i Ä‘Ã¢y náº¿u cáº§n

all_jobs = []

for page in range(start_page, end_page + 1):
    jobs = crawl_jobs_in_page(page)
    all_jobs.extend(jobs)

# ==== ğŸ“¦ Ghi ra file CSV ====
with open(f"tuyen_dung_vinhphuc_pages{end_page}.csv", "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.DictWriter(f, fieldnames=["title", "url", "company", "salary", "deadline", "location", "lien_he"])
    writer.writeheader()
    writer.writerows(all_jobs)

print(f"\nâœ… HoÃ n táº¥t! ÄÃ£ lÆ°u {len(all_jobs)} bÃ i Ä‘Äƒng vÃ o 'tuyen_dung_vinhphuc_pages.csv'")
