import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def extract_company_pair(name_block, info_block):
    name_vi = name_block.select_one("div.tendn")
    name_en = name_block.select_one("div.tengiaodich")
    info_items = info_block.select("li.info")

    data = {
        "TÃªn tiáº¿ng Viá»‡t": name_vi.text.replace("TÃªn tiáº¿ng Viá»‡t:", "").strip() if name_vi else "",
        "TÃªn tiáº¿ng Anh": name_en.text.replace("TÃªn tiáº¿ng Anh:", "").strip() if name_en else "",
        "Äá»‹a chá»‰": "",
        "Giáº¥y phÃ©p": "",
        "NgÃ y cáº¥p": "",
        "Äiá»‡n thoáº¡i": "",
        "Website": "",
        "Email": "",
        "Pháº¡m vi hoáº¡t Ä‘á»™ng": "",
    }

    for item in info_items:
        text = item.get_text(strip=True)
        if text.startswith("Äá»‹a chá»‰:"):
            data["Äá»‹a chá»‰"] = text.replace("Äá»‹a chá»‰:", "").strip()
        elif text.startswith("Giáº¥y phÃ©p"):
            data["Giáº¥y phÃ©p"] = text.split(":", 1)[1].strip()
        elif text.startswith("NgÃ y cáº¥p:"):
            data["NgÃ y cáº¥p"] = text.replace("NgÃ y cáº¥p:", "").strip()
        elif text.startswith("Äiá»‡n thoáº¡i:"):
            data["Äiá»‡n thoáº¡i"] = text.replace("Äiá»‡n thoáº¡i:", "").strip()
        elif text.startswith("Website:"):
            data["Website"] = text.replace("Website:", "").strip()
        elif text.startswith("Email:"):
            data["Email"] = text.replace("Email:", "").strip()
        elif text.startswith("Pháº¡m vi hoáº¡t Ä‘á»™ng:"):
            data["Pháº¡m vi hoáº¡t Ä‘á»™ng"] = text.replace("Pháº¡m vi hoáº¡t Ä‘á»™ng:", "").strip()

    return data

def crawl_pages_by_range(base_url, page_start, page_end):
    all_data = []
    for page in range(page_start, page_end + 1):
        url = f"{base_url}/{page}"
        print(f"ğŸ“„ Äang láº¥y trang {page}...")
        response = requests.get(url)
        if response.status_code != 200:
            print(f"âŒ Lá»—i khi táº£i trang {page}: {response.status_code}")
            continue

        soup = BeautifulSoup(response.text, "html.parser")

        name_blocks = soup.select("div.company-name.mar-bot")
        info_blocks = soup.select("div.thick-border")

        if not name_blocks or not info_blocks:
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u táº¡i trang {page}.")
            continue

        for name_block, info_block in zip(name_blocks, info_blocks):
            data = extract_company_pair(name_block, info_block)
            all_data.append(data)

        time.sleep(1)

    return all_data

# Gá»i hÃ m chÃ­nh
if __name__ == "__main__":
    base_url = "https://www.quanlyluhanh.vn/index.php/cat/1001" #quá»‘c táº¿
    # base_url = "https://www.quanlyluhanh.vn/index.php/cat/1020" #ná»™i Ä‘á»‹a

    # ğŸ‘‰ Äáº·t khoáº£ng trang báº¡n muá»‘n láº¥y á»Ÿ Ä‘Ã¢y
    page_start = 1
    page_end = 250

    all_companies = crawl_pages_by_range(base_url, page_start, page_end)

    df = pd.DataFrame(all_companies)
    df.to_excel(f"dulich_quanlyluhanh_page_{page_start}_to_{page_end}.xlsx", index=False)
    print(f"âœ… ÄÃ£ lÆ°u vÃ o file dulich_quanlyluhanh_page_{page_start}_to_{page_end}.xlsx")
