import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from urllib.parse import quote

headers = {
    "User-Agent": "Mozilla/5.0"
}

def get_updated_max(soup):
    articles = soup.find_all("article", class_="blog-post")
    if not articles:
        return None
    last_time_tag = articles[-1].find("time", class_="published")
    if last_time_tag:
        return last_time_tag["datetime"]
    return None

# Kho·∫£ng trang c·∫ßn l·∫•y
START_PAGE = 3
END_PAGE = 60

# Trang ƒë·∫ßu ti√™n
page = 1
next_url = "https://www.vieclamhalong.com/search?max-results=50"
all_posts = []
visited_links = set()

while page <= END_PAGE:
    print(f"üìÑ ƒêang x·ª≠ l√Ω trang {page}: {next_url}")
    res = requests.get(next_url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")

    articles = soup.find_all("article", class_="blog-post")
    if not articles:
        print("‚õî Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o. K·∫øt th√∫c.")
        break

    if page >= START_PAGE:
        for article in articles:
            a_tag = article.find("h2", class_="post-title").find("a")
            title = a_tag.get_text(strip=True)
            url = a_tag["href"]
            if url in visited_links:
                continue
            visited_links.add(url)

            date_tag = article.find("time", class_="published")
            post_date = date_tag["datetime"] if date_tag else "Kh√¥ng r√µ ng√†y"

            try:
                detail_res = requests.get(url, headers=headers, timeout=10)
                detail_soup = BeautifulSoup(detail_res.text, "html.parser")
                body_tag = detail_soup.find("div", class_="post-body post-content")
                content = body_tag.get_text(separator="\n", strip=True) if body_tag else "Kh√¥ng c√≥ n·ªôi dung"

                all_posts.append({
                    "Ti√™u ƒë·ªÅ": title,
                    "Link": url,
                    "Ng√†y ƒëƒÉng": post_date,
                    "N·ªôi dung chi ti·∫øt": content
                })

                print(f"‚úÖ Trang {page} | {title}")
                time.sleep(1)

            except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω b√†i: {url}: {e}")

    # L·∫•y updated-max ƒë·ªÉ truy c·∫≠p trang ti·∫øp theo
    updated_max = get_updated_max(soup)
    if not updated_max:
        print("‚ùå Kh√¥ng t√¨m th·∫•y updated-max, k·∫øt th√∫c.")
        break

    updated_time_encoded = quote(updated_max, safe='')
    next_url = f"https://www.vieclamhalong.com/search?updated-max={updated_time_encoded}&max-results=50"
    page += 1

# L∆∞u ra file Excel
df = pd.DataFrame(all_posts)
df.to_excel(f"vieclamhalong_page_{START_PAGE}_to_{END_PAGE}.xlsx", index=False)
print(f"üéâ ƒê√£ l∆∞u v√†o vieclamhalong_page_{START_PAGE}_to_{END_PAGE}.xlsx")
